{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHOP4o5ZBeyc","outputId":"c70bbfb1-2625-48c0-bdf1-29dad4a66736","executionInfo":{"status":"ok","timestamp":1733338158078,"user_tz":-330,"elapsed":6158,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n","Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.9.0.post1\n"]}],"source":["!pip install langchain transformers faiss-cpu requests\n"]},{"cell_type":"markdown","source":["http://127.0.0.1:4040"],"metadata":{"id":"MuazlVHFQb4f"}},{"cell_type":"code","source":["import requests\n","\n","# Configuration\n","LLM_SERVER_URL = \"https://1cce-2409-40f2-12b-13dd-9097-2df2-7a67-5e04.ngrok-free.app\"\n","ENDPOINT = f\"{LLM_SERVER_URL}/v1/chat/completions\"\n","\n","def query_llm(prompt: str) -> str:\n","    \"\"\"\n","    Send a query to the hosted LLM.\n","    \"\"\"\n","    payload = {\n","        \"model\": \"llama-3.2-1b-instruct\",\n","        \"messages\": [\n","            { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n","            { \"role\": \"user\", \"content\": prompt }\n","        ],\n","        \"temperature\": 0.7\n","    }\n","\n","    headers = {\"Content-Type\": \"application/json\"}\n","\n","    try:\n","        response = requests.post(ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an error for HTTP codes 4xx or 5xx\n","        data = response.json()\n","\n","        # Adjust response parsing based on your API format\n","        return data[\"choices\"][0][\"message\"][\"content\"]\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")\n","    except KeyError:\n","        raise Exception(f\"Unexpected response structure: {response.text}\")\n","\n","# Example usage\n","print(query_llm(\"What is the capital of India?\"))\n"],"metadata":{"id":"Cv12Q4NoKHAa","outputId":"cf164f8d-eb94-4e2d-edad-378353a04281","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1733338160129,"user_tz":-330,"elapsed":405,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}}},"execution_count":2,"outputs":[{"output_type":"error","ename":"Exception","evalue":"Request failed: 404 Client Error: Not Found for url: https://1cce-2409-40f2-12b-13dd-9097-2df2-7a67-5e04.ngrok-free.app/v1/chat/completions","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b6a60bfe73cd>\u001b[0m in \u001b[0;36mquery_llm\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Raise an error for HTTP codes 4xx or 5xx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://1cce-2409-40f2-12b-13dd-9097-2df2-7a67-5e04.ngrok-free.app/v1/chat/completions","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b6a60bfe73cd>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the capital of India?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-b6a60bfe73cd>\u001b[0m in \u001b[0;36mquery_llm\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"choices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Request failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected response structure: {response.text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Request failed: 404 Client Error: Not Found for url: https://1cce-2409-40f2-12b-13dd-9097-2df2-7a67-5e04.ngrok-free.app/v1/chat/completions"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ZlF4OzgFcVSB"}},{"cell_type":"code","source":["!pip install langchain faiss-cpu openai requests streamlit tiktoken\n","!pip install -U langchain-community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xaChZmfjdESH","executionInfo":{"status":"ok","timestamp":1733356126731,"user_tz":-330,"elapsed":23242,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"cf6679ce-ebac-462f-9163-2b220c66f736"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n","Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.40.2)\n","Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n","Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu, tiktoken\n","Successfully installed faiss-cpu-1.9.0.post1 tiktoken-0.8.0\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain<0.4.0,>=0.3.8 (from langchain-community)\n","  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.21 (from langchain-community)\n","  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.143)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (0.3.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (2.9.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (2.23.4)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n","Downloading langchain_community-0.3.9-py3-none-any.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n","Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.19\n","    Uninstalling langchain-core-0.3.19:\n","      Successfully uninstalled langchain-core-0.3.19\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.7\n","    Uninstalling langchain-0.3.7:\n","      Successfully uninstalled langchain-0.3.7\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.9 langchain-community-0.3.9 langchain-core-0.3.21 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"],"metadata":{"id":"pRZXA9CCf_L0","executionInfo":{"status":"ok","timestamp":1733338327250,"user_tz":-330,"elapsed":402,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","# Check OpenAIEmbeddings initialization\n","embedding = OpenAIEmbeddings(openai_api_key=\"your_openai_api_key\")\n","print(\"OpenAIEmbeddings initialized successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NleFzuxg2e8","executionInfo":{"status":"ok","timestamp":1733338335888,"user_tz":-330,"elapsed":2809,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"f680ac21-6d43-435c-9063-1c8fea966175"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-48a2a440d275>:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n","  embedding = OpenAIEmbeddings(openai_api_key=\"your_openai_api_key\")\n"]},{"output_type":"stream","name":"stdout","text":["OpenAIEmbeddings initialized successfully!\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","\n","json_folder = \"/content/dataset\"  # Path to your JSON files\n","\n","def inspect_json_files(json_folder):\n","    for filename in os.listdir(json_folder):\n","        if filename.endswith(\".json\"):\n","            file_path = os.path.join(json_folder, filename)\n","            try:\n","                with open(file_path, 'r') as f:\n","                    content = json.load(f)\n","                    print(f\"File: {filename}\")\n","                    print(json.dumps(content, indent=2))  # Pretty print the JSON content\n","                    print(\"\\n---\\n\")\n","            except json.JSONDecodeError:\n","                print(f\"Error decoding JSON in file: {filename}\")\n","            except Exception as e:\n","                print(f\"Unexpected error with file {filename}: {e}\")\n","\n","inspect_json_files(json_folder)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1NcsUF2OIFAOSbQ2i154LzneN5TdNqLue"},"collapsed":true,"id":"cRZ3Qbk-hKmf","executionInfo":{"status":"ok","timestamp":1733303298182,"user_tz":-330,"elapsed":1334,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"92b7e856-d6e0-4858-d048-485b06b4a7d6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import os\n","import json\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.schema import Document\n","\n","# Load JSON dataset\n","def load_dataset(json_folder: str):\n","    \"\"\"\n","    Load all JSON files from a folder into a list of Document objects.\n","    Extract relevant fields and combine them into a single text document.\n","    \"\"\"\n","    if not os.path.exists(json_folder):\n","        raise FileNotFoundError(f\"The dataset folder '{json_folder}' does not exist.\")\n","\n","    documents = []\n","    for filename in os.listdir(json_folder):\n","        if filename.endswith(\".json\"):\n","            file_path = os.path.join(json_folder, filename)\n","            try:\n","                with open(file_path, 'r') as f:\n","                    content = json.load(f)\n","\n","                    # Define relevant fields to extract and combine\n","                    relevant_fields = [\n","                        \"HIGHLIGHTS OF PRESCRIBING INFORMATION\",\n","                        \"1 INDICATIONS AND USAGE\",\n","                        \"2 DOSAGE AND ADMINISTRATION\",\n","                        \"3 DOSAGE FORMS AND STRENGTHS\",\n","                        \"4 CONTRAINDICATIONS\",\n","                        \"5 WARNINGS AND PRECAUTIONS\",\n","                        \"6 ADVERSE REACTIONS\",\n","                        \"7 DRUG INTERACTIONS\",\n","                        \"8 USE IN SPECIFIC POPULATIONS\",\n","                        \"10 OVERDOSAGE\",\n","                        \"11 DESCRIPTION\",\n","                        \"12 CLINICAL PHARMACOLOGY\",\n","                        \"13 NONCLINICAL TOXICOLOGY\",\n","                        \"14 CLINICAL STUDIES\",\n","                        \"15 REFERENCES\",\n","                        \"16 HOW SUPPLIED/STORAGE AND HANDLING\",\n","                        \"17 PATIENT COUNSELING INFORMATION\",\n","                        \"PACKAGE LABEL.PRINCIPAL DISPLAY PANEL\",\n","                        \"INGREDIENTS AND APPEARANCE\"\n","                    ]\n","\n","                    # Combine text from relevant fields\n","                    combined_text = \"\\n\".join(\n","                        content.get(field, \"\") for field in relevant_fields if field in content\n","                    )\n","\n","                    # Include metadata\n","                    metadata = {\n","                        \"product_name\": content.get(\"product_name\", \"\"),\n","                        \"source_file\": filename,\n","                        \"page\": content.get(\"page\", \"\"),\n","                    }\n","\n","                    # Add to documents if combined text is not empty\n","                    if combined_text.strip():\n","                        documents.append(Document(page_content=combined_text, metadata=metadata))\n","                    else:\n","                        print(f\"Skipping file '{file_path}' as it contains no relevant text.\")\n","            except json.JSONDecodeError:\n","                print(f\"Error decoding JSON in file: {filename}\")\n","            except Exception as e:\n","                print(f\"Unexpected error with file {filename}: {e}\")\n","    return documents\n","\n","# Create FAISS vector store\n","def create_vector_store(documents, api_key: str):\n","    \"\"\"\n","    Create a FAISS vector store for RAG using OpenAI embeddings.\n","    \"\"\"\n","    if not api_key or api_key == \"your_openai_api_key\":\n","        raise ValueError(\"A valid OpenAI API key is required. Set the key as an environment variable or pass it explicitly.\")\n","\n","    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n","    vectorstore = FAISS.from_documents(documents, embeddings)\n","    return vectorstore\n","\n","# Main script execution\n","if __name__ == \"__main__\":\n","    # Ensure OpenAI API key is set\n","    API_KEY = os.getenv(\"OPENAI_API_KEY\")\n","    if not API_KEY:\n","        raise ValueError(\"OPENAI_API_KEY is not set. Please provide a valid OpenAI API key.\")\n","\n","    # Set dataset path\n","    json_folder = \"/content/dataset\"  # Update with your dataset folder path\n","\n","    try:\n","        # Load documents\n","        docs = load_dataset(json_folder)\n","        print(f\"Loaded {len(docs)} documents from '{json_folder}'.\")\n","\n","        # Create vector store if documents are loaded\n","        if docs:\n","            vector_store = create_vector_store(docs, API_KEY)\n","            print(\"Vector store created successfully!\")\n","        else:\n","            print(\"No valid documents found. Vector store creation aborted.\")\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsaMTbfPfVE-","executionInfo":{"status":"ok","timestamp":1733303312203,"user_tz":-330,"elapsed":10536,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"03c20304-1a8c-4364-c1ab-46f07bfeabe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 3 documents from '/content/dataset'.\n","Vector store created successfully!\n"]}]},{"cell_type":"code","source":["# Save the FAISS vector store to disk\n","vector_store_path = \"/content/faiss_vector_store\"  # Specify your desired save path\n","vector_store.save_local(vector_store_path)\n","print(f\"Vector store saved to {vector_store_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgfJ9zHuixPH","executionInfo":{"status":"ok","timestamp":1733303317177,"user_tz":-330,"elapsed":353,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"05ed3096-4dd4-473c-dd47-cdf5a087dc61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector store saved to /content/faiss_vector_store\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","import shutil\n","import os\n","\n","# Zip the entire vector store folder\n","shutil.make_archive('faiss_vector_store', 'zip', vector_store_path)\n","\n","# Download the zipped folder\n","files.download('faiss_vector_store.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"8I2Pg60-7pSR","executionInfo":{"status":"ok","timestamp":1733303441283,"user_tz":-330,"elapsed":588,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"6e01f259-a3c0-44e8-8fb8-e473004358db"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_677e9c67-7c3b-4b1b-9758-e9792d37dfed\", \"faiss_vector_store.zip\", 74084)"]},"metadata":{}}]},{"cell_type":"code","source":["import os\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","# Ensure OpenAI API key is set\n","API_KEY = os.getenv(\"OPENAI_API_KEY\")\n","if not API_KEY:\n","    raise ValueError(\"OPENAI_API_KEY is not set. Please provide a valid OpenAI API key.\")\n","\n","# Path to the saved vector store\n","vector_store_path = \"/content/faiss_vector_store\"  # Update with your vector store path\n","\n","try:\n","    # Load the vector store with deserialization enabled\n","    embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)\n","    vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n","    print(\"Vector store loaded successfully!\")\n","\n","    # View the total number of vectors stored\n","    print(f\"Total vectors stored: {vector_store.index.ntotal}\")\n","\n","    # Inspect document contents and metadata\n","    print(\"\\nInspecting documents and metadata:\")\n","    for doc in vector_store.similarity_search(\"sample query\", k=5):  # Replace with your query\n","        print(\"Document Content:\")\n","        print(doc.page_content[:500])  # Show first 500 characters of the document\n","        print(\"\\nMetadata:\")\n","        print(doc.metadata)\n","        print(\"\\n---\\n\")\n","except Exception as e:\n","    print(f\"Error: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1a-VDwrpizs1","executionInfo":{"status":"ok","timestamp":1733303458660,"user_tz":-330,"elapsed":737,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"e9aa6854-b6e8-4dee-fe5e-f8f77ee35d85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector store loaded successfully!\n","Total vectors stored: 3\n","\n","Inspecting documents and metadata:\n","Document Content:\n","These highlights do not include all the information needed to use AMLODIPINE and OLMESARTAN MEDOXOMIL TABLETS safely and effectively. See full prescribing information for AMLODIPINE and OLMESARTAN MEDOXOMIL TABLETS.AMLODIPINE and OLMESARTAN MEDOXOMIL tablets, for oral useInitial U.S. Approval: 2007WARNING: FETAL TOXICITYSee full prescribing information for complete boxed warning.When pregnancy is detected, discontinue amlodipine and olmesartan medoxomil as soon as possible (5.1,8.1).Drugs that a\n","\n","Metadata:\n","{'product_name': 'Amlodipine Besylate and Olmesartan Medoxomil Tablets', 'source_file': 'Amlodipine Besylate and Olmesartan Medoxomil Tablets.json', 'page': ''}\n","\n","---\n","\n","Document Content:\n","These highlights do not include all the information needed to use AMOXICILLIN AND CLAVULANATE POTASSIUM FOR ORAL SUSPENSION safely and effectively. See full prescribing information for AMOXICILLIN AND CLAVULANATE POTASSIUM FOR ORAL SUSPENSION.AMOXICILLIN and CLAVULANATE POTASSIUM for oral suspensionInitial U.S. Approval: 1984RECENT MAJOR CHANGESWarnings and Precautions, Drug-Induced Enterocolitis Syndrome (DIES) (5.3) 5/2024INDICATIONS AND USAGEAmoxicillin and clavulanate potassium for oral susp\n","\n","Metadata:\n","{'product_name': 'Amoxicillin and Clavulanate Potassium for Oral Suspension, USP', 'source_file': 'Amoxicillin and Clavulanate Potassium for Oral Suspension, USP.json', 'page': ''}\n","\n","---\n","\n","Document Content:\n","NDC 42571-243-01acetaZOLAMIDEExtended-ReleaseCapsules500 mgRx Only100 TabletsMICRO LABS LIMITED\n","NDC 42571-243-01acetaZOLAMIDEExtended-ReleaseCapsules500 mgRx Only100 TabletsMICRO LABS LIMITED\n","NDC 42571-243-01acetaZOLAMIDEExtended-ReleaseCapsules500 mgRx Only100 TabletsMICRO LABS LIMITED\n","NDC 42571-243-01acetaZOLAMIDEExtended-ReleaseCapsules500 mgRx Only100 TabletsMICRO LABS LIMITED\n","NDC 42571-243-01acetaZOLAMIDEExtended-ReleaseCapsules500 mgRx Only100 TabletsMICRO LABS LIMITED\n","NDC 42571-243-01acet\n","\n","Metadata:\n","{'product_name': 'Acetazolamide Extended-Release Capsules', 'source_file': 'Acetazolamide Extended-Release Capsules.json', 'page': ''}\n","\n","---\n","\n"]}]},{"cell_type":"markdown","source":["WGWAGHWAGWAGWGWGWGWQGWQG"],"metadata":{"id":"dBueB2K58HDM"}},{"cell_type":"code","source":["import requests\n","import json\n","import re\n","from typing import Dict, Optional, Any\n","from pydantic import BaseModel, Field, ValidationError, model_validator\n","from langchain.tools import tool\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain_core.runnables import RunnableSequence\n","from langchain_core.runnables import RunnableSequence\n","\n","# Configuration\n","LLM_SERVER_URL = \"https://bf2c-122-172-84-113.ngrok-free.app\"\n","ENDPOINT = f\"{LLM_SERVER_URL}/v1/chat/completions\"\n","\n","\n","def query_llm(prompt: str) -> str:\n","    \"\"\"\n","    Send a query to the hosted LLM.\n","    \"\"\"\n","    payload = {\n","        \"model\": \"llama-3.2-1b-instruct\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0.7\n","    }\n","\n","    headers = {\"Content-Type\": \"application/json\"}\n","\n","    try:\n","        response = requests.post(ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an error for HTTP codes 4xx or 5xx\n","        data = response.json()\n","\n","        # Adjust response parsing based on your API format\n","        return data[\"choices\"][0][\"message\"][\"content\"]\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")\n","    except KeyError:\n","        raise Exception(f\"Unexpected response structure: {response.text}\")\n","\n","\n","class QueryAnalysis(BaseModel):\n","    symptoms: str = Field(default=\"\", description=\"Extracted symptoms from the query\")\n","    condition: str = Field(default=\"\", description=\"Extracted medical condition\")\n","    drugs: str = Field(default=\"\", description=\"Extracted drug names\")\n","    context: Optional[str] = Field(default=None, description=\"Additional context\")\n","\n","    @classmethod\n","    def parse_response(cls, response: str) -> 'QueryAnalysis':\n","        \"\"\"\n","        Robustly parse the LLM response into a QueryAnalysis object\n","        \"\"\"\n","        # Debugging print to see the raw response\n","        print(\"Raw response:\", response)\n","\n","        # Preprocessing: Remove code blocks and extra whitespace\n","        response = response.strip('`{}').strip()\n","\n","        try:\n","            # First, try JSON parsing\n","            try:\n","                parsed_dict = json.loads(response)\n","                return cls(**parsed_dict)\n","            except json.JSONDecodeError:\n","                # If JSON parsing fails, try more flexible parsing\n","                parsed_dict = {}\n","\n","                # Extract key-value pairs using regex with more flexible matching\n","                keys_to_extract = [\n","                    ('symptoms', r'symptoms?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('condition', r'conditions?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('drugs', r'drugs?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('context', r'contexts?:\\s*\"?([^\"\\n]+)\"?')\n","                ]\n","\n","                for key, pattern in keys_to_extract:\n","                    match = re.search(pattern, response, re.IGNORECASE)\n","                    if match:\n","                        parsed_dict[key] = match.group(1).strip()\n","                print(\"KEYS:\", keys_to_extract)\n","                # Fallback to empty values if no matches\n","                return cls(**parsed_dict)\n","\n","        except Exception as e:\n","            print(f\"Parsing error: {e}\")\n","\n","\n","# Configuration\n","LLM_SERVER_URL = \"https://7dfa-122-172-84-113.ngrok-free.app\"\n","ENDPOINT = f\"{LLM_SERVER_URL}/v1/chat/completions\"\n","def query_llm(prompt: str) -> str:\n","    \"\"\"\n","    Send a query to the hosted LLM.\n","    \"\"\"\n","    payload = {\n","        \"model\": \"llama-3.2-1b-instruct\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0.7\n","    }\n","\n","    headers = {\"Content-Type\": \"application/json\"}\n","\n","    try:\n","        response = requests.post(ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an error for HTTP codes 4xx or 5xx\n","        data = response.json()\n","\n","        # Adjust response parsing based on your API format\n","        return data[\"choices\"][0][\"message\"][\"content\"]\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")\n","    except KeyError:\n","        raise Exception(f\"Unexpected response structure: {response.text}\")\n","\n","class QueryAnalysis(BaseModel):\n","    symptoms: str = Field(default=\"\", description=\"Extracted symptoms from the query\")\n","    condition: str = Field(default=\"\", description=\"Extracted medical condition\")\n","    drugs: str = Field(default=\"\", description=\"Extracted drug names\")\n","    context: Optional[str] = Field(default=None, description=\"Additional context\")\n","\n","    @classmethod\n","    def parse_response(cls, response: str) -> 'QueryAnalysis':\n","        \"\"\"\n","        Robustly parse the LLM response into a QueryAnalysis object\n","        \"\"\"\n","        # Debugging print to see the raw response\n","        print(\"Raw response:\", response)\n","\n","        # Preprocessing: Remove code blocks and extra whitespace\n","        response = response.strip('`{}').strip()\n","\n","        try:\n","            # First, try JSON parsing\n","            try:\n","                parsed_dict = json.loads(response)\n","                return cls(**parsed_dict)\n","            except json.JSONDecodeError:\n","                # If JSON parsing fails, try more flexible parsing\n","                parsed_dict = {}\n","\n","                # Extract key-value pairs using regex with more flexible matching\n","                keys_to_extract = [\n","                    ('symptoms', r'symptoms?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('condition', r'conditions?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('drugs', r'drugs?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('context', r'contexts?:\\s*\"?([^\"\\n]+)\"?')\n","                ]\n","\n","                for key, pattern in keys_to_extract:\n","                    match = re.search(pattern, response, re.IGNORECASE)\n","                    if match:\n","                        parsed_dict[key] = match.group(1).strip()\n","                print(\"KEYS:\", keys_to_extract)\n","                # Fallback to empty values if no matches\n","                return cls(**parsed_dict)\n","\n","        except Exception as e:\n","            print(f\"Parsing error: {e}\")\n","            # Fallback to default if all parsing fails\n","            return cls()\n","\n","API_KEY=\"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\"\n","# Initialize embeddings (you can replace with your preferred embedding method)\n","embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)# Load the existing FAISS vector store from disk\n","vector_store_path = \"/content/faiss_vector_store\"\n","vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n","\n","@tool\n","def analyze_query_tool(query: str) -> str:\n","    \"\"\"\n","    Analyze the query to extract key entities.\n","    \"\"\"\n","    prompt = f\"\"\"\n","    Read the following medical query: '{query}' and give final ouput as only a JSON .\n","    First, classify it into one of these types and dont print it:\n","    a. direct queries or general queries\n","    b. recommendations or warnings\n","    c. summarize product details\n","    d. references to related products\n","    e. agent-based interaction design\n","\n","    Then, extract key entities such as symptoms, conditions, drugs, and context.\n","    Return the output as only a VALID JSON with these keys present in {query}:\n","    {{\n","        \"type\": \"one of the above types\",\n","        \"symptoms\": \"extracted symptoms\",\n","        \"condition\": \"extracted medical condition\",\n","        \"drugs\": \"extracted drug names\",\n","        \"context\": \"optional additional context\"\n","    }}\n","    \"\"\"\n","\n","    # Use the query_llm function directly\n","    response = query_llm(prompt)\n","    #print(\"response of analyze is:\", response)\n","    # Use the robust parsing method\n","    # res=QueryAnalysis.parse_response(response)\n","    #print(\"response of analyze is:\",res)\n","\n","    try:\n","        print(\"AWGWAGWG\",response)\n","        analysis = json.loads(response)\n","        return response\n","    except json.JSONDecodeError:\n","        raise Exception(f\"Invalid JSON response: {response}\")\n","    # print(json.loads(response))\n","    # return response\n","\n","\n","@tool\n","def retrieve_relevant_data_tool(analysis: str) -> str:\n","    \"\"\"\n","    Retrieve relevant medical data based on the analyzed query using the vector store.\n","    \"\"\"\n","    print(\"in retrieve relevant data\", analysis)\n","\n","    # Parse the analysis string to a dictionary\n","    analysis = json.loads(analysis)\n","    query_type = analysis.get(\"type\", \"\")\n","     # Construct a standardized search query for similarity scoring\n","    if analysis.get(\"type\", \"\") == \"c\":\n","        search_query = f\"Product details related to symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"d\":\n","        search_query = f\"References to related products for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"b\":\n","        search_query = f\"Medical recommendations or warnings for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"a\":\n","        search_query = f\"General medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    else:\n","        search_query = f\"Relevant medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","\n","\n","    # Construct search query\n","    # search_query = f\"Relevant medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, drugs: {analysis.get('drugs', '')}\"\n","\n","    # Perform similarity search and get only the most similar document\n","    docs = vector_store.similarity_search(search_query, k=3)\n","    #print(docs)\n","    # Get the first (most similar) document's content\n","    # context = docs[0].page_content if docs else \"\"\n","    context = \" \".join(doc.page_content for doc in docs)\n","\n","    # Truncate context to a manageable length (e.g., 1000 characters)\n","    context = context[:3000]\n","\n","    print(\"CHECKING CONTEXT:\", context)\n","    return context\n","@tool\n","def generate_recommendation_tool(context: str, query_type: str) -> str:\n","\n","    \"\"\"\n","    Generate a answer using the retrieved context and the LLM.\n","    \"\"\"\n","    print(\"LOUDEE\")\n","    # Tailor the LLM prompt based on the query type\n","    if query_type == \"b\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate medical recommendations or warnings relevant to the query.\n","        \"\"\"\n","    elif query_type == \"c\":\n","        prompt = f\"\"\"\n","        Based on the following product details:\n","        {context}\n","\n","        Provide a concise summary of the product.\n","        \"\"\"\n","    elif query_type == \"d\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Provide references to related products.\n","        \"\"\"\n","    elif query_type == \"a\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate a general medical response relevant to the query.\n","        \"\"\"\n","    else:\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate an appropriate response for the query type: {query_type}.\n","        \"\"\"\n","\n","    # Query the LLM with the tailored prompt\n","    recommendation = query_llm(prompt)\n","    print(\"Generated Recommendation:\", recommendation)\n","    return recommendation\n","\n","\n","# rag_chain = RunnableSequence(\n","#     analyze_query_tool |\n","#     retrieve_relevant_data_tool |\n","#     generate_recommendation_tool\n","# )\n","rag_chain = RunnableSequence(\n","    analyze_query_tool |\n","    (lambda analysis: retrieve_relevant_data_tool(analysis) |\n","    (lambda context, analysis: generate_recommendation_tool(context, analysis[\"type\"])))\n",")\n","\n","\n","# Example usage\n","def main():\n","    query = \"I have a headache and a history of stomach ulcers. Can you recommend a safe medication?\"\n","    try:\n","        recommendation = rag_chain.invoke(query)\n","        print(\"Recommendation:\", recommendation)\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"d9iNAtpm8umF","executionInfo":{"status":"ok","timestamp":1733360530303,"user_tz":-330,"elapsed":2058,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"adadf6ae-4d7a-4ed7-8013-aff136bfb8ef"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["AWGWAGWG {\n","  \"type\": \"b\",\n","  \"symptoms\": {\n","    \"headache\"\n","  },\n","  \"condition\": {\n","    \"medical condition\": \"history of stomach ulcers\"\n","  },\n","  \"drugs\": []\n","}\n","An error occurred: Invalid JSON response: {\n","  \"type\": \"b\",\n","  \"symptoms\": {\n","    \"headache\"\n","  },\n","  \"condition\": {\n","    \"medical condition\": \"history of stomach ulcers\"\n","  },\n","  \"drugs\": []\n","}\n"]}]},{"cell_type":"markdown","source":["AWINFWIHGOUAWGBIWAGUIW"],"metadata":{"id":"GDZx6zPz8HkY"}},{"cell_type":"code","source":["import requests\n","import json\n","import re\n","from typing import Dict, Optional, Any, Tuple\n","from pydantic import BaseModel, Field, ValidationError, model_validator\n","from langchain.tools import tool\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain_core.runnables import RunnableSequence\n","\n","# Configuration\n","LLM_SERVER_URL = \"https://7dfa-122-172-84-113.ngrok-free.app\"\n","ENDPOINT = f\"{LLM_SERVER_URL}/v1/chat/completions\"\n","\n","def query_llm(prompt: str) -> str:\n","    \"\"\"\n","    Send a query to the hosted LLM.\n","    \"\"\"\n","    payload = {\n","        \"model\": \"llama-3.2-1b-instruct\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0.7\n","    }\n","\n","    headers = {\"Content-Type\": \"application/json\"}\n","\n","    try:\n","        response = requests.post(ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an error for HTTP codes 4xx or 5xx\n","        data = response.json()\n","\n","        # Adjust response parsing based on your API format\n","        return data[\"choices\"][0][\"message\"][\"content\"]\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")\n","    except KeyError:\n","        raise Exception(f\"Unexpected response structure: {response.text}\")\n","\n","class QueryAnalysis(BaseModel):\n","    symptoms: str = Field(default=\"\", description=\"Extracted symptoms from the query\")\n","    condition: str = Field(default=\"\", description=\"Extracted medical condition\")\n","    drugs: str = Field(default=\"\", description=\"Extracted drug names\")\n","    context: Optional[str] = Field(default=None, description=\"Additional context\")\n","\n","    @classmethod\n","    def parse_response(cls, response: str) -> 'QueryAnalysis':\n","        \"\"\"\n","        Robustly parse the LLM response into a QueryAnalysis object\n","        \"\"\"\n","        # Debugging print to see the raw response\n","        print(\"Raw response:\", response)\n","\n","        # Preprocessing: Remove code blocks and extra whitespace\n","        response = response.strip('`{}').strip()\n","\n","        try:\n","            # First, try JSON parsing\n","            try:\n","                parsed_dict = json.loads(response)\n","                return cls(**parsed_dict)\n","            except json.JSONDecodeError:\n","                # If JSON parsing fails, try more flexible parsing\n","                parsed_dict = {}\n","\n","                # Extract key-value pairs using regex with more flexible matching\n","                keys_to_extract = [\n","                    ('symptoms', r'symptoms?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('condition', r'conditions?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('drugs', r'drugs?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('context', r'contexts?:\\s*\"?([^\"\\n]+)\"?')\n","                ]\n","\n","                for key, pattern in keys_to_extract:\n","                    match = re.search(pattern, response, re.IGNORECASE)\n","                    if match:\n","                        parsed_dict[key] = match.group(1).strip()\n","                print(\"KEYS:\", keys_to_extract)\n","                # Fallback to empty values if no matches\n","                return cls(**parsed_dict)\n","\n","        except Exception as e:\n","            print(f\"Parsing error: {e}\")\n","            # Fallback to default if all parsing fails\n","            return cls()\n","\n","API_KEY=\"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\"\n","# Initialize embeddings (you can replace with your preferred embedding method)\n","embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)\n","# Load the existing FAISS vector store from disk\n","vector_store_path = \"/content/faiss_vector_store\"\n","vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n","\n","@tool\n","def analyze_query_tool(query: str) -> str:\n","    \"\"\"\n","    Analyze the query to extract key entities.\n","    \"\"\"\n","    prompt = f\"\"\"\n","    Read the following medical query: '{query}' and give final ouput as only a JSON .\n","    First, classify it into one of these types such as a or b or c etc and dont print it:\n","    a. direct queries or general queries\n","    b. recommendations or warnings\n","    c. summarize product details\n","    d. references to related products\n","    e. agent-based interaction design\n","\n","    Then, extract key entities such as symptoms, conditions, drugs, and context.\n","    Return the output as only a VALID JSON with these keys present in {query}:\n","    {{\n","        \"type\": \"one of the above types\",\n","        \"symptoms\": \"extracted symptoms\",\n","        \"condition\": \"extracted medical condition\",\n","        \"drugs\": \"extracted drug names\",\n","        \"context\": \"optional additional context\"\n","    }}\n","    \"\"\"\n","\n","    # Use the query_llm function directly\n","    response = query_llm(prompt)\n","\n","    try:\n","        print(\"AWGWAGWG\",response)\n","        # analysis = json.loads(response)\n","        return response\n","    except json.JSONDecodeError:\n","        raise Exception(f\"LOUDA JSON response: {response}\")\n","\n","@tool\n","def retrieve_relevant_data_tool(analysis: str) ->  Dict[str, Any]:\n","    \"\"\"\n","    Retrieve relevant medical data based on the analyzed query using the vector store.\n","    \"\"\"\n","    print(\"in retrieve relevant data\", analysis)\n","    analysis=json.loads(analysis)\n","    query_type = analysis.get(\"type\", \"\")\n","    # Construct a standardized search query for similarity scoring\n","    if analysis.get(\"type\", \"\") == \"c\":\n","        search_query = f\"Product details related to symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"d\":\n","        search_query = f\"References to related products for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"b\":\n","        search_query = f\"Medical recommendations or warnings for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"a\":\n","        search_query = f\"General medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    else:\n","        search_query = f\"Relevant medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","\n","    # Perform similarity search and get only the most similar document\n","    docs = vector_store.similarity_search(search_query, k=3)\n","    context = \" \".join(doc.page_content for doc in docs)\n","\n","    # Truncate context to a manageable length (e.g., 3000 characters)\n","    context = context[:3000]\n","\n","    print(\"CHECKING CONTEXT:\", context)\n","    return json.dumps({\"context\": context, \"analysis\": analysis})\n","\n","@tool\n","def generate_recommendation_tool(data: str) -> str:\n","    \"\"\"\n","    Generate a answer using the retrieved context and the LLM.\n","    \"\"\"\n","    data = json.loads(data)\n","    context = data.get(\"context\", \"\")\n","    analysis = data.get(\"analysis\", {})\n","    query_type = analysis.get(\"type\", \"\")\n","    # Tailor the LLM prompt based on the query type\n","    if query_type == \"b\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate medical recommendations or warnings relevant to the query.\n","        \"\"\"\n","    elif query_type == \"c\":\n","        prompt = f\"\"\"\n","        Based on the following product details:\n","        {context}\n","\n","        Provide a concise summary of the product.\n","        \"\"\"\n","    elif query_type == \"d\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Provide references to related products.\n","        \"\"\"\n","    elif query_type == \"a\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate a general medical response relevant to the query.\n","        \"\"\"\n","    else:\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate an appropriate response for the query type: {query_type}.\n","        \"\"\"\n","\n","    # Query the LLM with the tailored prompt\n","    recommendation = query_llm(prompt)\n","    print(\"Generated Recommendation:\", recommendation)\n","    return recommendation\n","\n","# Correct the chaining of functions\n","rag_chain = RunnableSequence(\n","    analyze_query_tool |\n","    retrieve_relevant_data_tool |\n","    generate_recommendation_tool\n",")\n","\n","# Example usage\n","def main():\n","    query = \"I have a headache and a history of stomach ulcers. Can you recommend a safe medication?\"\n","    try:\n","        recommendation = rag_chain.invoke(query)\n","        print(\"Recommendation:\", recommendation)\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kr25O17MV7CO","executionInfo":{"status":"ok","timestamp":1733361294678,"user_tz":-330,"elapsed":36341,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"f48f98ac-2e8c-460e-f8f0-e1159bd415b7"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["AWGWAGWG {\n","  \"type\": \"b\",\n","  \"symptoms\": \"headache and a history of stomach ulcers\",\n","  \"condition\": \"stomach ulcer\"\n","}\n","in retrieve relevant data {\n","  \"type\": \"b\",\n","  \"symptoms\": \"headache and a history of stomach ulcers\",\n","  \"condition\": \"stomach ulcer\"\n","}\n","CHECKING CONTEXT: These highlights do not include all the information needed to use AMLODIPINE and OLMESARTAN MEDOXOMIL TABLETS safely and effectively. See full prescribing information for AMLODIPINE and OLMESARTAN MEDOXOMIL TABLETS.AMLODIPINE and OLMESARTAN MEDOXOMIL tablets, for oral useInitial U.S. Approval: 2007WARNING: FETAL TOXICITYSee full prescribing information for complete boxed warning.When pregnancy is detected, discontinue amlodipine and olmesartan medoxomil as soon as possible (5.1,8.1).Drugs that act directly on the renin-angiotensin system can cause injury and death to the developing fetus (5.1,8.1).INDICATIONS AND USAGEAmlodipine and olmesartan medoxomil tablet is a combination of amlodipine besylate, a dihydropyridine calcium channel blocker, and olmesartan medoxomil, an angiotensin II receptor blocker, indicated for the treatment of hypertension, alone or with other antihypertensive agents, to lower blood pressure. Lowering blood pressure reduces the risk of fatal and nonfatal cardiovascular events, primarily strokes and myocardial infarctions (1).Amlodipine and olmesartan medoxomil tablets may also be used as initial therapy in patients likely to need multiple antihypertensive agents to achieve their blood pressure goals (1).DOSAGE AND ADMINISTRATIONRecommended starting dose: 5/20 mg once daily (2).Titrate as needed in two-week intervals up to a maximum of 10/40 mg once daily (2).DOSAGE FORMS AND STRENGTHSTablets: (amlodipine/olmesartan medoxomil content) 5/20 mg, 10/20 mg, 5/40 mg, and 10/40 mg (3).CONTRAINDICATIONSDo not co-administer aliskiren with amlodipine and olmesartan medoxomil tablets in patients with diabetes (4).WARNINGS AND PRECAUTIONSAnticipate hypotension in volume-or salt-depleted patients with treatment initiation. Start treatment under close supervision (5.2).Increased angina or myocardial infarction may occur upon dosage initiation or increase (5.3).Impaired renal function: changes in renal function may occur (5.4).Sprue-like enteropathy has been reported. Consider discontinuation of amlodipine and olmesartan medoxomil in cases where no other etiology is found (5.6).ADVERSE REACTIONSMost common adverse reaction (incidence ≥3%) is edema (6.1).To report SUSPECTED ADVERSE REACTIONS, contactMicro Labs USA, Inc. at 1-855-839-8195 orFDA at 1-800-332-1088 orwww.fda.gov/medwatch.DRUG INTERACTIONSAmlodipine (7.1):If simvastatin is co-administered with amlodipine, do not exceed 20 mg daily of simvastatin.Increased exposure of cyclosporine and tacrolimus.Increased exposure of amlodipine when coadministered with CYP3A inhibitors.Olmesartan medoxomil (7.2):Nonsteroidal anti-inflammatory drugs (NSAIDS) may lead to increased risk of renal impairment and loss of antihypertensive effect.Dual inhibition of the renin-angiotensin system: Increased risk of renal impairment, hypotension, and hyperkalemia.Colesevelam hydrochloride: Consider administering olmesartan at least 4 hours before colesevelam hydrochloride dose.Lithium: Increases in serum l\n","Generated Recommendation: Based on the provided context, here are some medical recommendations and warnings:\n","\n","**Warnings**\n","\n","1. **Fetal toxicity**: Pregnancy is contraindicated with amlodipine and olmesartan medoxomil tablets, especially when detected early.\n","2. **Increased angina or myocardial infarction**: Dosage initiation or increase may occur in patients with cardiovascular disease.\n","3. **Impaired renal function**: Changes in renal function can occur when using amlodipine and olmesartan medoxomil tablets.\n","\n","**Recommendations**\n","\n","1. **Close monitoring**: Treatment should be initiated under close supervision to anticipate hypotension in volume-or salt-depleted patients.\n","2. **Increased caution with simvastatin co-administration**: Amlodipine and simvastatin cannot exceed 20 mg daily without risk of increased exposure of cyclosporine and tacrolimus.\n","\n","**General Guidelines**\n","\n","1. **Avoid concurrent use**: Aliskiren should not be co-administered with amlodipine and olmesartan medoxomil tablets in patients with diabetes.\n","2. **Consider discontinuation**: Sprue-like enteropathy has been reported; consider discontinuing the treatment if no other etiology is found.\n","\n","These recommendations are based on the provided context, which highlights potential risks associated with the combination of amlodipine and olmesartan medoxomil tablets in certain patients, such as pregnant women, individuals with impaired renal function, or those taking simvastatin.\n","Recommendation: Based on the provided context, here are some medical recommendations and warnings:\n","\n","**Warnings**\n","\n","1. **Fetal toxicity**: Pregnancy is contraindicated with amlodipine and olmesartan medoxomil tablets, especially when detected early.\n","2. **Increased angina or myocardial infarction**: Dosage initiation or increase may occur in patients with cardiovascular disease.\n","3. **Impaired renal function**: Changes in renal function can occur when using amlodipine and olmesartan medoxomil tablets.\n","\n","**Recommendations**\n","\n","1. **Close monitoring**: Treatment should be initiated under close supervision to anticipate hypotension in volume-or salt-depleted patients.\n","2. **Increased caution with simvastatin co-administration**: Amlodipine and simvastatin cannot exceed 20 mg daily without risk of increased exposure of cyclosporine and tacrolimus.\n","\n","**General Guidelines**\n","\n","1. **Avoid concurrent use**: Aliskiren should not be co-administered with amlodipine and olmesartan medoxomil tablets in patients with diabetes.\n","2. **Consider discontinuation**: Sprue-like enteropathy has been reported; consider discontinuing the treatment if no other etiology is found.\n","\n","These recommendations are based on the provided context, which highlights potential risks associated with the combination of amlodipine and olmesartan medoxomil tablets in certain patients, such as pregnant women, individuals with impaired renal function, or those taking simvastatin.\n"]}]},{"cell_type":"code","source":["!pip install streamlit pyngrok\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FOw2mjQWbmu","executionInfo":{"status":"ok","timestamp":1733356096374,"user_tz":-330,"elapsed":6185,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"7f829ff0-4fe6-4a4e-993e-1ae248e282d7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting pyngrok\n","  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n","Collecting watchdog<7,>=2.1.5 (from streamlit)\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n","Downloading streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n","Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 pyngrok-7.2.1 streamlit-1.40.2 watchdog-6.0.0\n"]}]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","\n","# Set the title of the Streamlit app\n","st.title(\"Medical Query Recommendation System\")\n","\n","# Create a text input for the user to enter their query\n","query = st.text_input(\"Enter your medical query:\")\n","\n","# Create a button to submit the query\n","if st.button(\"Get Recommendation\"):\n","    if query:\n","        try:\n","            # Invoke the RAG chain with the user's query\n","            recommendation = rag_chain.invoke(query)\n","            # Display the recommendation\n","            st.success(\"Recommendation:\")\n","            st.write(recommendation)\n","        except Exception as e:\n","            st.error(f\"An error occurred: {e}\")\n","    else:\n","        st.warning(\"Please enter a query.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C359Jb_yWcNG","executionInfo":{"status":"ok","timestamp":1733311371367,"user_tz":-330,"elapsed":374,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"b4e79d16-9de0-4a62-c75f-faaa9b84f223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","import os\n","\n","# Set your ngrok authtoken\n","ngrok_authtoken = \"2pkWoemvNfmephCosgYXd2higYP_3dLR85rUT5znPCB86s4HF\"  # Replace with your actual ngrok authtoken\n","os.environ[\"NGROK_AUTHTOKEN\"] = ngrok_authtoken\n","\n","# Kill any existing ngrok tunnels\n","ngrok.kill()\n","\n","# Start ngrok tunnel\n","public_url = ngrok.connect(8501)  # Use integer port number\n","print(f\"Streamlit app is running at: {public_url}\")\n","\n","# Run the Streamlit app\n","!streamlit run app.py &>/dev/null&\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHLZNDYsWf72","executionInfo":{"status":"ok","timestamp":1733311379199,"user_tz":-330,"elapsed":676,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"0a24d1ff-65e5-422b-dde3-3ca292ad3064"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit app is running at: NgrokTunnel: \"https://0cff-34-106-246-245.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"hm5V3ciuYcgK"}},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","import requests\n","import json\n","import re\n","from typing import Dict, Optional, Any\n","from pydantic import BaseModel, Field, ValidationError, model_validator\n","from langchain.tools import tool\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain_core.runnables import RunnableSequence\n","\n","# Configuration\n","LLM_SERVER_URL = \"https://bf2c-122-172-84-113.ngrok-free.app\"\n","ENDPOINT = f\"{LLM_SERVER_URL}/v1/chat/completions\"\n","\n","def query_llm(prompt: str) -> str:\n","    \"\"\"\n","    Send a query to the hosted LLM.\n","    \"\"\"\n","    payload = {\n","        \"model\": \"llama-3.2-1b-instruct\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0.7\n","    }\n","\n","    headers = {\"Content-Type\": \"application/json\"}\n","\n","    try:\n","        response = requests.post(ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an error for HTTP codes 4xx or 5xx\n","        data = response.json()\n","\n","        # Adjust response parsing based on your API format\n","        return data[\"choices\"][0][\"message\"][\"content\"]\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")\n","    except KeyError:\n","        raise Exception(f\"Unexpected response structure: {response.text}\")\n","\n","\n","API_KEY = \"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\"\n","# Initialize embeddings (you can replace with your preferred embedding method)\n","embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)\n","# Load the existing FAISS vector store from disk\n","vector_store_path = \"/content/faiss_vector_store\"\n","vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n","\n","@tool\n","def analyze_query_tool(query: str) -> str:\n","    \"\"\"\n","    Analyze the query to extract key entities (e.g., drugs, symptoms, conditions).\n","    \"\"\"\n","    prompt = f\"\"\"Analyze the following medical query: '{query}'.\n","    Identify key entities such as drugs, symptoms, conditions, and context.\n","    Return the output as a VALID JSON with these exact keys:\n","    {{\n","        \"symptoms\": \"extracted symptoms\",\n","        \"condition\": \"extracted medical condition\",\n","        \"drugs\": \"extracted drug names\",\n","        \"context\": \"optional additional context\"\n","    }}\n","\n","    If no specific entities are found, use empty strings. Provide just the json\"\"\"\n","\n","    # Use the query_llm function directly\n","    response = query_llm(prompt)\n","    #print(\"response of analyze is:\", response)\n","    # Use the robust parsing method\n","    # res=QueryAnalysis.parse_response(response)\n","    #print(\"response of analyze is:\",res)\n","    print(json.loads(response))\n","    return response\n","\n","@tool\n","def retrieve_relevant_data_tool(analysis: str) -> str:\n","    \"\"\"\n","    Retrieve relevant medical data based on the analyzed query using the vector store.\n","    \"\"\"\n","    print(\"in retrieve relevant data\", analysis)\n","    # Parse the analysis string to a dictionary\n","    analysis = json.loads(analysis)\n","\n","    # Construct search query\n","    search_query = f\"Give a few Relevant medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, drugs: {analysis.get('drugs', '')}\"\n","\n","    # Perform similarity search and get only the most similar document\n","    docs = vector_store.similarity_search(search_query, k=1)\n","\n","    # Get the first (most similar) document's content\n","    context = docs[0].page_content if docs else \"\"\n","\n","    # Truncate context to a manageable length (e.g., 1000 characters)\n","    context = context[:3000]\n","\n","    print(\"CHECKING CONTEXT:\", context)\n","    return context\n","\n","@tool\n","def generate_recommendation_tool(context: str) -> str:\n","    \"\"\"\n","    Generate a answer using the retrieved context and the LLM.\n","    \"\"\"\n","    prompt = f\"\"\"Given the following medical context:\n","    {context}\n","\n","    Give answer using above context and the query\"\"\"\n","    # print(prompt)\n","    # Use the query_llm function directly\n","    recommendation = query_llm(prompt)\n","    print(\"recommendation\", recommendation)\n","    return recommendation\n","\n","rag_chain = RunnableSequence(\n","    analyze_query_tool |\n","    retrieve_relevant_data_tool |\n","    generate_recommendation_tool\n",")\n","\n","# Set the title of the Streamlit app\n","st.title(\"Medical Query Recommendation System\")\n","\n","# Create a text input for the user to enter their query\n","query = st.text_input(\"Enter your medical query:\")\n","\n","# Create a button to submit the query\n","if st.button(\"Get Result\"):\n","    if query:\n","        try:\n","            # Invoke the RAG chain with the user's query\n","            recommendation = rag_chain.invoke(query)\n","            # Display the recommendation\n","            st.success(\"Recommendation:\")\n","            st.write(recommendation)\n","        except Exception as e:\n","            st.error(f\"An error occurred: {e}\")\n","    else:\n","        st.warning(\"Please enter a query.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HEPjnLN8WszN","executionInfo":{"status":"ok","timestamp":1733311727852,"user_tz":-330,"elapsed":381,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"5a502191-0f5c-4a9c-d59c-ed1f8a84b7d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","source":["V1.2"],"metadata":{"id":"_DaEQIeKDRzg"}},{"cell_type":"code","source":["import streamlit as st\n","u78iremport requests\n","import json\n","import re\n","from typing import Dict, Optional, Any\n","from pydantic import BaseModel, Field, ValidationError, model_validator\n","from langchain.tools import tool\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain_core.runnables import RunnableSequence\n","\n","# Configuration\n","LLM_SERVER_URL = \"https://bf2c-122-172-84-113.ngrok-free.app\"\n","ENDPOINT = f\"{LLM_SERVER_URL}/v1/chat/completions\"\n","\n","def query_llm(prompt: str) -> str:\n","    \"\"\"\n","    Send a query to the hosted LLM.\n","    \"\"\"\n","    payload = {\n","        \"model\": \"llama-3.2-1b-instruct\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0.7\n","    }\n","\n","    headers = {\"Content-Type\": \"application/json\"}\n","\n","    try:\n","        response = requests.post(ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an error for HTTP codes 4xx or 5xx\n","        data = response.json()\n","\n","        # Adjust response parsing based on your API format\n","        return data[\"choices\"][0][\"message\"][\"content\"]\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")\n","    except KeyError:\n","        raise Exception(f\"Unexpected response structure: {response.text}\")\n","\n","\n","API_KEY = \"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\"\n","# Initialize embeddings (you can replace with your preferred embedding method)\n","embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)\n","# Load the existing FAISS vector store from disk\n","vector_store_path = \"/content/faiss_vector_store\"\n","vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n","\n","@tool\n","def analyze_query_tool(query: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Analyze the query to classify its type and extract key entities.\n","    \"\"\"\n","    prompt = f\"\"\"\n","    Analyze the following medical query: '{query}'.\n","    First, classify it into one of these types:\n","    a. direct queries or general queries\n","    b. recommendations or warnings\n","    c. summarize product details\n","    d. references to related products\n","    e. agent-based interaction design\n","\n","    Then, extract key entities such as symptoms, conditions, drugs, and context.\n","    Return the output as a VALID JSON with these keys:\n","    {{\n","        \"type\": \"one of the above types\",\n","        \"symptoms\": \"extracted symptoms\",\n","        \"condition\": \"extracted medical condition\",\n","        \"drugs\": \"extracted drug names\",\n","        \"context\": \"optional additional context\"\n","    }}\n","    \"\"\"\n","\n","    response = query_llm(prompt)\n","    try:\n","        analysis = json.loads(response)\n","        return analysis\n","    except json.JSONDecodeError:\n","        raise Exception(f\"Invalid JSON response: {response}\")\n","\n","@tool\n","def retrieve_relevant_data_tool(analysis: Dict[str, Any]) -> str:\n","    \"\"\"\n","    Retrieve relevant data based on the analysis and type of query.\n","    \"\"\"\n","    query_type = analysis.get(\"type\", \"\")\n","    symptoms = analysis.get(\"symptoms\", \"\")\n","    condition = analysis.get(\"condition\", \"\")\n","    drugs = analysis.get(\"drugs\", \"\")\n","\n","    if query_type == \"summarize product details\":\n","        search_query = f\"Summarize product details for symptoms: {symptoms}, condition: {condition}, drugs: {drugs}.\"\n","    elif query_type == \"references to related products\":\n","        search_query = f\"Find references for related products to symptoms: {symptoms}, condition: {condition}, drugs: {drugs}.\"\n","    else:\n","        search_query = f\"Give relevant medical information for symptoms: {symptoms}, condition: {condition}, drugs: {drugs}.\"\n","\n","    docs = vector_store.similarity_search(search_query, k=1)\n","    context = docs[0].page_content if docs else \"\"\n","    return context[:3000]\n","\n","@tool\n","def generate_recommendation_tool(context: str, query_type: str) -> str:\n","    \"\"\"\n","    Generate a tailored recommendation or response based on the context and query type.\n","    \"\"\"\n","    if query_type in [\"recommendations or warnings\", \"direct queries or general queries\"]:\n","        prompt = f\"\"\"Given the following context:\n","        {context}\n","\n","        Generate a response tailored for {query_type}.\"\"\"\n","    else:\n","        prompt = f\"\"\"Given the following context:\n","        {context}\n","\n","        Provide a response relevant to the query type: {query_type}.\"\"\"\n","\n","    recommendation = query_llm(prompt)\n","    return recommendation\n","\n","# Update RAG Chain\n","rag_chain = RunnableSequence(\n","    analyze_query_tool |\n","    (lambda analysis: retrieve_relevant_data_tool(analysis) |\n","    (lambda context, analysis: generate_recommendation_tool(context, analysis[\"type\"])))\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"jt5Pq1LeEN-9","executionInfo":{"status":"error","timestamp":1733356074868,"user_tz":-330,"elapsed":911,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"7c90639b-7275-4082-c5c1-5ea513eaee0e"},"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-1-25450679ceb2>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-25450679ceb2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    u78iremport requests\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","source":["V1.3\n"],"metadata":{"id":"kDDp_JETEO8x"}},{"cell_type":"code","source":["%%writefile app.py\n","import requests\n","import json\n","import re\n","from typing import Dict, Optional, Any, Tuple\n","from pydantic import BaseModel, Field, ValidationError, model_validator\n","from langchain.tools import tool\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","from langchain_core.runnables import RunnableSequence\n","import streamlit as st  # Import Streamlit\n","\n","# Configuration\n","LLM_SERVER_URL = \"https://7dfa-122-172-84-113.ngrok-free.app\"\n","ENDPOINT = f\"{LLM_SERVER_URL}/v1/chat/completions\"\n","\n","def query_llm(prompt: str) -> str:\n","    \"\"\"\n","    Send a query to the hosted LLM.\n","    \"\"\"\n","    payload = {\n","        \"model\": \"llama-3.2-1b-instruct\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0.7\n","    }\n","\n","    headers = {\"Content-Type\": \"application/json\"}\n","\n","    try:\n","        response = requests.post(ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an error for HTTP codes 4xx or 5xx\n","        data = response.json()\n","\n","        # Adjust response parsing based on your API format\n","        return data[\"choices\"][0][\"message\"][\"content\"]\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")\n","    except KeyError:\n","        raise Exception(f\"Unexpected response structure: {response.text}\")\n","\n","class QueryAnalysis(BaseModel):\n","    symptoms: str = Field(default=\"\", description=\"Extracted symptoms from the query\")\n","    condition: str = Field(default=\"\", description=\"Extracted medical condition\")\n","    drugs: str = Field(default=\"\", description=\"Extracted drug names\")\n","    context: Optional[str] = Field(default=None, description=\"Additional context\")\n","\n","    @classmethod\n","    def parse_response(cls, response: str) -> 'QueryAnalysis':\n","        \"\"\"\n","        Robustly parse the LLM response into a QueryAnalysis object\n","        \"\"\"\n","        # Debugging print to see the raw response\n","        print(\"Raw response:\", response)\n","\n","        # Preprocessing: Remove code blocks and extra whitespace\n","        response = response.strip('`{}').strip()\n","\n","        try:\n","            # First, try JSON parsing\n","            try:\n","                parsed_dict = json.loads(response)\n","                return cls(**parsed_dict)\n","            except json.JSONDecodeError:\n","                # If JSON parsing fails, try more flexible parsing\n","                parsed_dict = {}\n","\n","                # Extract key-value pairs using regex with more flexible matching\n","                keys_to_extract = [\n","                    ('symptoms', r'symptoms?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('condition', r'conditions?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('drugs', r'drugs?:\\s*\"?([^\"\\n]+)\"?'),\n","                    ('context', r'contexts?:\\s*\"?([^\"\\n]+)\"?')\n","                ]\n","\n","                for key, pattern in keys_to_extract:\n","                    match = re.search(pattern, response, re.IGNORECASE)\n","                    if match:\n","                        parsed_dict[key] = match.group(1).strip()\n","                print(\"KEYS:\", keys_to_extract)\n","                # Fallback to empty values if no matches\n","                return cls(**parsed_dict)\n","\n","        except Exception as e:\n","            print(f\"Parsing error: {e}\")\n","            # Fallback to default if all parsing fails\n","            return cls()\n","\n","API_KEY=\"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\"\n","# Initialize embeddings (you can replace with your preferred embedding method)\n","embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)\n","# Load the existing FAISS vector store from disk\n","vector_store_path = \"/content/faiss_vector_store\"\n","vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n","\n","@tool\n","def analyze_query_tool(query: str) -> str:\n","    \"\"\"\n","    Analyze the query to extract key entities.\n","    \"\"\"\n","    prompt = f\"\"\"\n","    Read the following medical query: '{query}' and give final ouput as only a JSON .\n","    First, classify it into one of these types such as a or b or c etc and dont print it:\n","    a. direct queries or general queries\n","    b. recommendations or warnings\n","    c. summarize product details\n","    d. references to related products\n","    e. agent-based interaction design\n","\n","    Then, extract key entities such as symptoms, conditions, drugs, and context.\n","    Return the output as only a VALID JSON with these keys present in {query}:\n","    {{\n","        \"type\": \"one of the above types\",\n","        \"symptoms\": \"extracted symptoms\",\n","        \"condition\": \"extracted medical condition\",\n","        \"drugs\": \"extracted drug names\",\n","        \"context\": \"optional additional context\"\n","    }}\n","    \"\"\"\n","\n","    # Use the query_llm function directly\n","    response = query_llm(prompt)\n","\n","    try:\n","        print(\"AWGWAGWG\",response)\n","        # analysis = json.loads(response)\n","        return response\n","    except json.JSONDecodeError:\n","        raise Exception(f\"LOUDA JSON response: {response}\")\n","\n","@tool\n","def retrieve_relevant_data_tool(analysis: str) ->  Dict[str, Any]:\n","    \"\"\"\n","    Retrieve relevant medical data based on the analyzed query using the vector store.\n","    \"\"\"\n","    print(\"in retrieve relevant data\", analysis)\n","    analysis=json.loads(analysis)\n","    query_type = analysis.get(\"type\", \"\")\n","    # Construct a standardized search query for similarity scoring\n","    if analysis.get(\"type\", \"\") == \"c\":\n","        search_query = f\"Product details related to symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"d\":\n","        search_query = f\"References to related products for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"b\":\n","        search_query = f\"Medical recommendations or warnings for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    elif analysis.get(\"type\", \"\") == \"a\":\n","        search_query = f\"General medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","    else:\n","        search_query = f\"Relevant medical information for symptoms: {analysis.get('symptoms', '')}, condition: {analysis.get('condition', '')}, and drugs: {analysis.get('drugs', '')}.\"\n","\n","    # Perform similarity search and get only the most similar document\n","    docs = vector_store.similarity_search(search_query, k=3)\n","    context = \" \".join(doc.page_content for doc in docs)\n","\n","    # Truncate context to a manageable length (e.g., 3000 characters)\n","    context = context[:3000]\n","\n","    print(\"CHECKING CONTEXT:\", context)\n","    return json.dumps({\"context\": context, \"analysis\": analysis})\n","\n","@tool\n","def generate_recommendation_tool(data: str) -> str:\n","    \"\"\"\n","    Generate a answer using the retrieved context and the LLM.\n","    \"\"\"\n","    data = json.loads(data)\n","    context = data.get(\"context\", \"\")\n","    analysis = data.get(\"analysis\", {})\n","    query_type = analysis.get(\"type\", \"\")\n","    # Tailor the LLM prompt based on the query type\n","    if query_type == \"b\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate medical recommendations or warnings relevant to the query.\n","        \"\"\"\n","    elif query_type == \"c\":\n","        prompt = f\"\"\"\n","        Based on the following product details:\n","        {context}\n","\n","        Provide a concise summary of the product.\n","        \"\"\"\n","    elif query_type == \"d\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Provide references to related products.\n","        \"\"\"\n","    elif query_type == \"a\":\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate a general medical response relevant to the query.\n","        \"\"\"\n","    else:\n","        prompt = f\"\"\"\n","        Based on the following context:\n","        {context}\n","\n","        Generate an appropriate response for the query type: {query_type}.\n","        \"\"\"\n","\n","    # Query the LLM with the tailored prompt\n","    recommendation = query_llm(prompt)\n","    print(\"Generated Recommendation:\", recommendation)\n","    return recommendation\n","\n","# Correct the chaining of functions\n","rag_chain = RunnableSequence(\n","    analyze_query_tool |\n","    retrieve_relevant_data_tool |\n","    generate_recommendation_tool\n",")\n","\n","# Set the title of the Streamlit app\n","st.title(\"Medical Query Recommendation System\")\n","\n","# Create a text input for the user to enter their query\n","query = st.text_input(\"Enter your medical query:\")\n","\n","# Create a button to submit the query\n","if st.button(\"Get Result\"):\n","    if query:\n","        try:\n","            # Invoke the RAG chain with the user's query\n","            recommendation = rag_chain.invoke(query)\n","            # Display the recommendation\n","            st.success(\"Result:\")\n","            st.write(recommendation)\n","        except Exception as e:\n","            st.error(f\"An error occurred: {e}\")\n","    else:\n","        st.warning(\"Please enter a query.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JXAQTCZZDS48","executionInfo":{"status":"ok","timestamp":1733361570867,"user_tz":-330,"elapsed":703,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"2c2d847f-226c-4c08-bbee-7b3a776709b4"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","import os\n","\n","# Set your ngrok authtoken\n","ngrok_authtoken = \"2pkWoemvNfmephCosgYXd2higYP_3dLR85rUT5znPCB86s4HF\"  # Replace with your actual ngrok authtoken\n","os.environ[\"NGROK_AUTHTOKEN\"] = ngrok_authtoken\n","\n","# Kill any existing ngrok tunnels\n","ngrok.kill()\n","\n","# Start ngrok tunnel\n","public_url = ngrok.connect(8501)  # Use integer port number\n","print(f\"Streamlit app is running at: {public_url}\")\n","\n","# Run the Streamlit app\n","!streamlit run app.py &>/dev/null&\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tR-4IEf6bDEk","executionInfo":{"status":"ok","timestamp":1733361491578,"user_tz":-330,"elapsed":694,"user":{"displayName":"Rahul HR","userId":"17188233390244858064"}},"outputId":"90947d61-c660-4bd0-dd8b-77ef23aba977"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit app is running at: NgrokTunnel: \"https://f810-34-45-167-243.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-TNcjNJNbGKC"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCQPNmS5sbJaNAS8RIu2s6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}